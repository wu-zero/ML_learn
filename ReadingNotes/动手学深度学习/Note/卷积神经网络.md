# 目录
- [目录](#目录)
- [卷积神经网络基础](#卷积神经网络基础)
  - [内容](#内容)
  - [收获](#收获)
    - [理论](#理论)
      - [padding和stride](#padding和stride)
      - [1x1卷积](#1x1卷积)
    - [代码](#代码)
      - [`torch.Tensor`四种乘法](#torchtensor四种乘法)
      - [`nn.Conv2d`用法](#nnconv2d用法)
      - [`nn.MaxPool2d`用法](#nnmaxpool2d用法)
- [LeNet](#lenet)
  - [内容](#内容-1)
  - [收获](#收获-1)
    - [理论](#理论-1)
    - [代码](#代码-1)
      - [`with torch.no_grad()`用法](#with-torchno_grad用法)
- [卷积神经网络进阶](#卷积神经网络进阶)
  - [内容](#内容-2)
  - [收获](#收获-2)
    - [理论](#理论-2)
      - [全局平均池化](#全局平均池化)
    - [代码](#代码-2)
- [批量归一化](#批量归一化)
  - [内容](#内容-3)
  - [收获](#收获-3)
    - [理论](#理论-3)
      - [批量归一化](#批量归一化-1)
    - [代码](#代码-3)
- [残差网络](#残差网络)
  - [内容](#内容-4)
  - [收获](#收获-4)
    - [理论](#理论-4)
    - [代码](#代码-4)
# 卷积神经网络基础
## 内容
介绍卷积神经网络的基础概念，主要是卷积层和池化层，并解释填充、步幅、输入通道和输出通道的含义。
## 收获
### 理论
#### padding和stride
如果原输入的高和宽是$n_h$和$n_w$，卷积核的高和宽是$k_h$和$k_w$，在高的两侧一共填充$p_h$行，在宽的两侧一共填充$p_w$列，当高上步幅为$s_h$，宽上步幅为$s_w$时，输出形状为：
$$
\lfloor(n_h+p_h-k_h+s_h)/s_h\rfloor \times \lfloor(n_w+p_w-k_w+s_w)/s_w\rfloor
$$
如果$p_h=k_h-1$，$p_w=k_w-1$，那么输出形状将简化为$\lfloor(n_h+s_h-1)/s_h\rfloor \times \lfloor(n_w+s_w-1)/s_w\rfloor$。更进一步，如果输入的高和宽能分别被高和宽上的步幅整除，那么输出形状将是$(n_h / s_h) \times (n_w/s_w)$。
当$p_h = p_w = p$时，我们称填充为$p$；当$s_h = s_w = s$时，我们称步幅为$s$。

#### 1x1卷积
[参考](https://zhuanlan.zhihu.com/p/40050371)

### 代码
#### `torch.Tensor`四种乘法
* `*` 
  按元素做乘法，包含广播机制。
* `torch.mul` 
  同上
* `torch.mm` 
  矩阵乘法，不包含广播机制
* `torch.matmul`
  矩阵乘法，包含广播机制

#### `nn.Conv2d`用法
我们使用Pytorch中的`nn.Conv2d`类来实现二维卷积层，主要关注以下几个构造函数参数：

* `in_channels` (python:int) – Number of channels in the input imag
* `out_channels` (python:int) – Number of channels produced by the convolution
* `kernel_size` (python:int or tuple) – Size of the convolving kernel
* `stride` (python:int or tuple, optional) – Stride of the convolution. Default: 1
* `padding` (python:int or tuple, optional) – Zero-padding added to both sides of the input. Default: 0
* `bias` (bool, optional) – If True, adds a learnable bias to the output. Default: True

`forward`函数的参数为一个四维张量，形状为$(N, C_{in}, H_{in}, W_{in})$，返回值也是一个四维张量，形状为$(N, C_{out}, H_{out}, W_{out})$，其中$N$是批量大小，$C, H, W$分别表示通道数、高度、宽度。

#### `nn.MaxPool2d`用法
我们使用Pytorch中的`nn.MaxPool2d`实现最大池化层，关注以下构造函数参数：

* `kernel_size` – the size of the window to take a max over
* `stride` – the stride of the window. Default value is kernel_size
* `padding` – implicit zero padding to be added on both sides

`forward`函数的参数为一个四维张量，形状为$(N, C, H_{in}, W_{in})$，返回值也是一个四维张量，形状为$(N, C, H_{out}, W_{out})$，其中$N$是批量大小，$C, H, W$分别表示通道数、高度、宽度。
# LeNet
## 内容
1. LeNet模型介绍
2. LeNet网络搭建
3. 运用LeNet进行图像识别-fashion-mnist数据集
## 收获
### 理论
略
### 代码
#### `with torch.no_grad()`用法
验证模型时不需要求导，即不需要梯度计算，关闭autograd，可以提高速度，节约内存。如果不关闭可能会爆显存。
```python
with torch.no_grad():
    # 使用model进行预测的代码
    pass
```
# 卷积神经网络进阶
## 内容
1.AlexNet
2.VGG
3.NiN
4.GoogLeNet
## 收获
### 理论
#### 全局平均池化
全局平均池化就是平均池化的滑窗size和整张feature map的size一样大。这样，每个$W×H×C$的feature map输入就会被转化为$1×1×C$输出。因此，其实也等同于每个位置权重都为$1/(W×H)1$的FC层操作。
### 代码
略

# 批量归一化
## 内容
1. 对全连接层做批量归一化
2. 对卷积层做批量归⼀化
3. 预测时的批量归⼀化
## 收获 
### 理论
#### 批量归一化
BN步骤主要分为4步：
1. 求每一个训练批次数据的均值
2. 求每一个训练批次数据的方差
3. 使用求得的均值和方差对该批次的训练数据做归一化，获得0-1分布。其中ε是为了避免除数为0时所使用的微小正数。
4. 尺度变换和偏移：将$x_i$乘以$\gamma$调整数值大小，再加上$\beta$增加偏移后得到$y_i$，这里的$\gamma$是尺度因子，$\beta$是平移因子。这一步是BN的精髓，由于归一化后的xi基本会被限制在正态分布下，使得网络的表达能力下降。为解决该问题，我们引入两个新的参数：$\beta$,$\gamma$。$\beta$和$\gamma$是在训练时网络自己学习得到的。

[参考](https://www.jianshu.com/p/380bd53c713c)
### 代码
略
# 残差网络
## 内容
1. ResNet
2. DenseNet
## 收获 
### 理论
略
### 代码
略
