# 目录
- [目录](#目录)
- [机器翻译及相关技术](#机器翻译及相关技术)
  - [内容](#内容)
  - [收获](#收获)
    - [理论](#理论)
      - [Encoder-Decoder](#encoder-decoder)
      - [Beam Search](#beam-search)
    - [代码](#代码)
- [注意力机制与Seq2seq模型](#注意力机制与seq2seq模型)
  - [内容](#内容-1)
  - [收获](#收获-1)
    - [理论](#理论-1)
    - [代码](#代码-1)
- [Transformer](#transformer)
  - [内容](#内容-2)
  - [收获](#收获-2)
    - [理论](#理论-2)
    - [代码](#代码-2)
# 机器翻译及相关技术
## 内容
机器翻译（MT）：将一段文本从一种语言自动翻译为另一种语言，用神经网络解决这个问题通常称为神经机器翻译（NMT）。
主要特征：输出是单词序列而不是单个单词。 输出序列的长度可能与源序列的长度不同。
## 收获
### 理论
#### Encoder-Decoder
encoder：输入到隐藏状态  
decoder：隐藏状态到输出
#### Beam Search
* beam search只在预测的时候需要
* [参考](https://www.zhihu.com/question/54356960/answer/375588742)
### 代码
略
# 注意力机制与Seq2seq模型
## 内容
1. 注意力机制
2. 点积注意力
3. 多层感知机注意力
4. 引入注意力机制的Seq2Seq模型
## 收获
### 理论
略
### 代码
略
# Transformer
## 内容
Transformer同样基于编码器-解码器架构，其区别主要在于以下三点：
1. Transformer blocks：将seq2seq模型重的循环网络替换为了Transformer Blocks，该模块包含一个多头注意力层（Multi-head Attention Layers）以及两个position-wise feed-forward networks（FFN）。对于解码器来说，另一个多头注意力层被用于接受编码器的隐藏状态。  
2. Add and norm：多头注意力层和前馈网络的输出被送到两个“add and norm”层进行处理，该层包含残差结构以及层归一化。  
3. Position encoding：由于自注意力层并没有区分元素的顺序，所以一个位置编码层被用于向序列元素里添加位置信息。  
## 收获
### 理论
略
### 代码
略