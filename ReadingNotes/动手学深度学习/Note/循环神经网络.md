# 目录
- [目录](#目录)
- [文本预处理](#文本预处理)
  - [内容](#内容)
  - [收获](#收获)
    - [理论](#理论)
    - [代码](#代码)
      - [分词工具](#分词工具)
- [语言模型与数据集](#语言模型与数据集)
  - [内容](#内容-1)
  - [收获](#收获-1)
    - [理论](#理论-1)
      - [n元语法](#n元语法)
      - [时序数据的采样](#时序数据的采样)
    - [代码](#代码-1)
- [循环神经网络基础](#循环神经网络基础)
  - [内容](#内容-2)
  - [收获](#收获-2)
    - [理论](#理论-2)
      - [梯度剪裁](#梯度剪裁)
    - [代码](#代码-2)
      - [`tensor.scatter_()`用法](#tensorscatter_用法)
      - [`nn.RNN`用法](#nnrnn用法)
# 文本预处理
## 内容
文本是一类序列数据，一篇文章可以看作是字符或单词的序列，文本数据的常见预处理步骤，预处理通常包括四个步骤：  
1. 读入文本  
2. 分词  
3. 建立字典，将每个词映射到一个唯一的索引（index）  
4. 将文本从词的序列转换为索引的序列，方便输入模型  
## 收获
### 理论
略
### 代码
#### 分词工具
[spaCy](https://spacy.io/)和[NLTK](https://www.nltk.org/)

# 语言模型与数据集
## 内容
一段自然语言文本可以看作是一个离散时间序列，给定一个长度为$T$的词的序列$w_1, w_2, \ldots, w_T$，语言模型的目标就是评估该序列是否合理，即计算该序列的概率：

$$
P(w_1, w_2, \ldots, w_T).
$$

本节介绍基于统计的语言模型，主要是$n$元语法（$n$-gram）。
## 收获
### 理论
#### n元语法
[参考](https://blog.csdn.net/wangyangzhizhou/article/details/78651397)
#### 时序数据的采样
1. 随机采样
2. 相邻采样
### 代码
略
# 循环神经网络基础
## 内容
1. 循环神经网络的基本知识
2. 使用循环神经网络的从零开始的实现
3. 使用pytorch的简洁实现
## 收获
### 理论
#### 梯度剪裁
[参考](https://wulc.me/2018/05/01/%E6%A2%AF%E5%BA%A6%E8%A3%81%E5%89%AA%E5%8F%8A%E5%85%B6%E4%BD%9C%E7%94%A8/)
### 代码
#### `tensor.scatter_()`用法
`scatter_(dim, index, src) → Tensor`
Writes all values from the tensor src into self at the indices specified in the index tensor. For each value in src, its output index is specified by its index in src for dimension != dim and by the corresponding value in index for dimension = dim.

For a 3-D tensor, self is updated as:
```python
self[index[i][j][k]][j][k] = src[i][j][k]  # if dim == 0
self[i][index[i][j][k]][k] = src[i][j][k]  # if dim == 1
self[i][j][index[i][j][k]] = src[i][j][k]  # if dim == 2
```
用于onehot编码：
```python
def one_hot(x, n_class, dtype=torch.float32):
    result = torch.zeros(x.shape[0], n_class, dtype=dtype, device=x.device)  # shape: (n, n_class)
    result.scatter_(1, x.long().view(-1, 1), 1)  # result[i, x[i, 0]] = 1
    return result
```
#### `nn.RNN`用法
我们主要关注`nn.RNN`的以下几个构造函数参数：

* `input_size` - The number of expected features in the input x
* `hidden_size` – The number of features in the hidden state h
* `nonlinearity` – The non-linearity to use. Can be either 'tanh' or 'relu'. Default: 'tanh'
* `batch_first` – If True, then the input and output tensors are provided as (batch_size, num_steps, input_size). Default: False

`forward`函数的参数为：

* `input` of shape (num_steps, batch_size, input_size): tensor containing the features of the input sequence. 
* `h_0` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the initial hidden state for each element in the batch. Defaults to zero if not provided. If the RNN is bidirectional, num_directions should be 2, else it should be 1.

`forward`函数的返回值是：

* `output` of shape (num_steps, batch_size, num_directions * hidden_size): tensor containing the output features (h_t) from the last layer of the RNN, for each t.
* `h_n` of shape (num_layers * num_directions, batch_size, hidden_size): tensor containing the hidden state for t = num_steps.
