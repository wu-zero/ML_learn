# 目录
- [目录](#目录)
- [线性回归](#线性回归)
  - [内容](#内容)
  - [收获](#收获)
    - [理论](#理论)
    - [代码](#代码)
      - [创建`tensor`](#创建tensor)
      - [修改`tensor`维度](#修改tensor维度)
      - [定义模型](#定义模型)
- [Softmax与分类模型](#softmax与分类模型)
  - [内容](#内容-1)
  - [收获](#收获-1)
    - [理论](#理论-1)
      - [多类分类下为什么用softmax而不是用其他归一化方法？](#多类分类下为什么用softmax而不是用其他归一化方法)
      - [softmax和交叉熵相关推导](#softmax和交叉熵相关推导)
    - [代码](#代码-1)
      - [`tensor.gather()`用法](#tensorgather用法)
- [多层感知机](#多层感知机)
  - [内容](#内容-2)
  - [收获](#收获-2)
    - [理论](#理论-2)
      - [激活函数选择](#激活函数选择)
    - [代码](#代码-2)
# 线性回归
## 内容
主要内容包括：  
* 线性回归的基本要素  
* 线性回归模型从零开始的实现  
* 线性回归模型使用pytorch的简洁实现  
## 收获
### 理论
略
### 代码
#### 创建`tensor`
常见创建`tensor`的方法：  

|函数|功能|
|:---:|:---:|
|`Tensor(*sizes)`|基础构造函数|
|`tensor(data,)`|类似np.array的构造函数|
|`ones(*sizes)`|全1Tensor|
|`zeros(*sizes)`|全0Tensor|
|`eye(*sizes)`|对角线为1，其他为0|
|`arange(s,e,step)`|从s到e，步长为step|
|`linspace(s,e,steps)`|从s到e，均匀切分成steps份|
|`rand/randn(*sizes)`|均匀/标准分布|
|`normal(mean,std)/uniform(from,to)`|正态分布/均匀分布|
|`randperm(m)`|随机排列|

这些创建方法都可以在创建的时候指定数据类型`dtype`和存放`device(cpu/gpu)`。  
其中使用`Tensor`函数新建`tensor`是最复杂多变的方式，它既可以接收一个`list`，并根据`list`的数据新建`tensor`，也能根据指定的形状新建`tensor`,还能传入其他的`tensor`。  

#### 修改`tensor`维度
* `tensor.view()`
* `tensor.unsqueeze()`，`tensor.squeeze()`

#### 定义模型
1. 继承`nn.Module`
    `torch.nn`的核心数据结构是`Module`，它是一个抽象概念，既可以表示神经网络中的某个层，也可以表示一个包含很多层的神经网络。  
    需要注意以下几点：  
    * 必须继承`nn.Module`，并且在其构造函数中需调用`nn.Module`的构造函数。
    * 在构造函数`__init__`中必须自己定义可学习的参数，并封装成`Parameter`。parameter是一种特殊的`Tensor`，但其默认需要求导`（requires_grad = True）`。
    * `forward`函数实现前向传播过程，其输入可以是一个或多个`tensor`。
    * 无需写反向传播函数，`nn.Module`能够利用`autograd`自动实现反向传播。
    * 使用时，直观上可将`net`看成数学概念中的函数，调用`net(input)`即可得到`input`对应的结果。它等价于`net.__call__(input)`，在`__call__`函数中，主要调用的是 net.forward(x)，另外还对钩子做了一些处理。所以在实际使用中应尽量使用`net(x)`而不是使用`net.forward(x)`。
    * `Module`中的可学习参数可以通过`named_parameters()`或者`parameters()`返回迭代器，前者会给每个`parameter`都附上名字，使其更具有辨识度。
2. 使用`nn.Sequential`(是一个特殊的`Module`)
    ```python
    # Sequential的三种写法
    # method one
    net = nn.Sequential(
        nn.Linear(num_inputs, 1)
        # other layers can be added here
        )

    # method two
    net = nn.Sequential()
    net.add_module('linear', nn.Linear(num_inputs, 1))
    # net.add_module ......

    # method three
    from collections import OrderedDict
    net = nn.Sequential(OrderedDict([
              ('linear', nn.Linear(num_inputs, 1))
              # ......
            ]))
    ```
# Softmax与分类模型
## 内容
内容包含：  
1. softmax回归的基本概念  
2. 如何获取Fashion-MNIST数据集和读取数据  
3. softmax回归模型的从零开始实现，实现一个对Fashion-MNIST训练集中的图像数据进行分类的模型  
4. 使用pytorch重新实现softmax回归模型  
## 收获
### 理论
#### 多类分类下为什么用softmax而不是用其他归一化方法？
1.softmax设计的初衷，是希望特征对概率的影响是乘性的。
2.多类分类问题的目标函数常常选为交叉熵（cross-entropy），反向传播求导时两者化简。[参考](https://www.zhihu.com/question/40403377/answer/86783636)。
#### softmax和交叉熵相关推导
[参考](https://zhuanlan.zhihu.com/p/25723112)
### 代码
#### `tensor.gather()`用法
`torch.gather(input, dim, index, out=None, sparse_grad=False) → Tensor`
Gathers values along an axis specified by dim.
For a 3-D tensor the output is specified by:
```python
out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
```
用于计算交叉熵：
```python
def cross_entropy(y_hat, y):
    return - torch.log(y_hat.gather(1, y.view(-1, 1)))
```
# 多层感知机
## 内容
1. 多层感知机的基本知识  
2. 使用多层感知机图像分类的从零开始的实现  
3. 使用pytorch的简洁实现  
## 收获
### 理论
#### 激活函数选择
[参考](https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit%E3%80%82)  
[参考](https://blog.csdn.net/u011754972/article/details/81584719)
### 代码
略