# 目录
- [目录](#目录)
- [线性回归](#线性回归)
  - [内容](#内容)
  - [收获](#收获)
    - [理论](#理论)
    - [代码](#代码)
      - [创建`tensor`](#创建tensor)
      - [修改`tensor`维度](#修改tensor维度)
      - [定义模型](#定义模型)
- [Softmax与分类模型](#softmax与分类模型)
  - [内容](#内容-1)
  - [收获](#收获-1)
    - [理论](#理论-1)
      - [多类分类下为什么用softmax而不是用其他归一化方法？](#多类分类下为什么用softmax而不是用其他归一化方法)
      - [softmax和交叉熵相关推导](#softmax和交叉熵相关推导)
    - [代码](#代码-1)
      - [`tensor.gather()`用法](#tensorgather用法)
- [多层感知机](#多层感知机)
  - [内容](#内容-2)
  - [收获](#收获-2)
    - [理论](#理论-2)
      - [激活函数选择](#激活函数选择)
    - [代码](#代码-2)
- [过拟合、欠拟合及其解决方案](#过拟合欠拟合及其解决方案)
  - [内容](#内容-3)
  - [收获](#收获-3)
    - [理论](#理论-3)
      - [应对过拟合](#应对过拟合)
    - [代码](#代码-3)
      - [`model.train()`和`model.veal()`](#modeltrain和modelveal)
- [梯度消失、梯度爆炸以及Kaggle房价预测](#梯度消失梯度爆炸以及kaggle房价预测)
  - [内容](#内容-4)
  - [收获](#收获-4)
    - [理论](#理论-4)
    - [代码](#代码-4)
      - [`Pytorch`中的参数初始化方法](#pytorch中的参数初始化方法)
# 线性回归
## 内容
主要内容包括：  
* 线性回归的基本要素  
* 线性回归模型从零开始的实现  
* 线性回归模型使用pytorch的简洁实现  
## 收获
### 理论
略
### 代码
#### 创建`tensor`
常见创建`tensor`的方法：  

|函数|功能|
|:---:|:---:|
|`Tensor(*sizes)`|基础构造函数|
|`tensor(data,)`|类似np.array的构造函数|
|`ones(*sizes)`|全1Tensor|
|`zeros(*sizes)`|全0Tensor|
|`eye(*sizes)`|对角线为1，其他为0|
|`arange(s,e,step)`|从s到e，步长为step|
|`linspace(s,e,steps)`|从s到e，均匀切分成steps份|
|`rand/randn(*sizes)`|均匀/标准分布|
|`normal(mean,std)/uniform(from,to)`|正态分布/均匀分布|
|`randperm(m)`|随机排列|

这些创建方法都可以在创建的时候指定数据类型`dtype`和存放`device(cpu/gpu)`。  
其中使用`Tensor`函数新建`tensor`是最复杂多变的方式，它既可以接收一个`list`，并根据`list`的数据新建`tensor`，也能根据指定的形状新建`tensor`,还能传入其他的`tensor`。  

#### 修改`tensor`维度
* `tensor.view()`
* `tensor.unsqueeze()`，`tensor.squeeze()`

#### 定义模型
1. 继承`nn.Module`
    `torch.nn`的核心数据结构是`Module`，它是一个抽象概念，既可以表示神经网络中的某个层，也可以表示一个包含很多层的神经网络。  
    需要注意以下几点：  
    * 必须继承`nn.Module`，并且在其构造函数中需调用`nn.Module`的构造函数。
    * 在构造函数`__init__`中必须自己定义可学习的参数，并封装成`Parameter`。parameter是一种特殊的`Tensor`，但其默认需要求导`（requires_grad = True）`。
    * `forward`函数实现前向传播过程，其输入可以是一个或多个`tensor`。
    * 无需写反向传播函数，`nn.Module`能够利用`autograd`自动实现反向传播。
    * 使用时，直观上可将`net`看成数学概念中的函数，调用`net(input)`即可得到`input`对应的结果。它等价于`net.__call__(input)`，在`__call__`函数中，主要调用的是 net.forward(x)，另外还对钩子做了一些处理。所以在实际使用中应尽量使用`net(x)`而不是使用`net.forward(x)`。
    * `Module`中的可学习参数可以通过`named_parameters()`或者`parameters()`返回迭代器，前者会给每个`parameter`都附上名字，使其更具有辨识度。
2. 使用`nn.Sequential`(是一个特殊的`Module`)
    ```python
    # Sequential的三种写法
    # method one
    net = nn.Sequential(
        nn.Linear(num_inputs, 1)
        # other layers can be added here
        )

    # method two
    net = nn.Sequential()
    net.add_module('linear', nn.Linear(num_inputs, 1))
    # net.add_module ......

    # method three
    from collections import OrderedDict
    net = nn.Sequential(OrderedDict([
              ('linear', nn.Linear(num_inputs, 1))
              # ......
            ]))
    ```
# Softmax与分类模型
## 内容
内容包含：  
1. softmax回归的基本概念  
2. 如何获取Fashion-MNIST数据集和读取数据  
3. softmax回归模型的从零开始实现，实现一个对Fashion-MNIST训练集中的图像数据进行分类的模型  
4. 使用pytorch重新实现softmax回归模型  
## 收获
### 理论
#### 多类分类下为什么用softmax而不是用其他归一化方法？
1.softmax设计的初衷，是希望特征对概率的影响是乘性的。
2.多类分类问题的目标函数常常选为交叉熵（cross-entropy），反向传播求导时两者化简。[参考](https://www.zhihu.com/question/40403377/answer/86783636)。
#### softmax和交叉熵相关推导
[参考](https://zhuanlan.zhihu.com/p/25723112)
### 代码
#### `tensor.gather()`用法
`torch.gather(input, dim, index, out=None, sparse_grad=False) → Tensor`
Gathers values along an axis specified by dim.
For a 3-D tensor the output is specified by:
```python
out[i][j][k] = input[index[i][j][k]][j][k]  # if dim == 0
out[i][j][k] = input[i][index[i][j][k]][k]  # if dim == 1
out[i][j][k] = input[i][j][index[i][j][k]]  # if dim == 2
```
用于计算交叉熵：
```python
def cross_entropy(y_hat, y):
    return - torch.log(y_hat.gather(1, y.view(-1, 1)))
```
# 多层感知机
## 内容
1. 多层感知机的基本知识  
2. 使用多层感知机图像分类的从零开始的实现  
3. 使用pytorch的简洁实现  
## 收获
### 理论
#### 激活函数选择
[参考](https://zhuanlan.zhihu.com/p/21462488?refer=intelligentunit%E3%80%82)  
[参考](https://blog.csdn.net/u011754972/article/details/81584719)
### 代码
略
# 过拟合、欠拟合及其解决方案
## 内容
1. 过拟合、欠拟合的概念
2. 权重衰减
3. 丢弃法
## 收获
### 理论
#### 应对过拟合
* 权重衰减($L_2$ 范数正则化)
$L_2$范数正则化在模型原损失函数基础上添加$L_2$范数惩罚项，从而得到训练所需要最小化的函数。$L_2$范数惩罚项指的是模型权重参数每个元素的平方和与一个正的常数的乘积。以线性回归中的线性回归损失函数为例
$$
 \ell(w_1, w_2, b) = \frac{1}{n} \sum_{i=1}^n \frac{1}{2}\left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right)^2 
$$
其中$w_1, w_2$是权重参数，$b$是偏差参数，样本$i$的输入为$x_1^{(i)}, x_2^{(i)}$，标签为$y^{(i)}$，样本数为$n$。将权重参数用向量$\boldsymbol{w} = [w_1, w_2]$表示，带有$L_2$范数惩罚项的新损失函数为
$$
\ell(w_1, w_2, b) + \frac{\lambda}{2n} |\boldsymbol{w}|^2,
$$
其中超参数$\lambda > 0$。当权重参数均为0时，惩罚项最小。当$\lambda$较大时，惩罚项在损失函数中的比重较大，这通常会使学到的权重参数的元素较接近0。当$\lambda$设为0时，惩罚项完全不起作用。上式中$L_2$范数平方$|\boldsymbol{w}|^2$展开后得到$w_1^2 + w_2^2$。
有了$L_2$范数惩罚项后，在小批量随机梯度下降中，我们将线性回归一节中权重$w_1$和$w_2$的迭代方式更改为
$$
 \begin{aligned} w_1 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_1 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_1^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right),\\ w_2 &\leftarrow \left(1- \frac{\eta\lambda}{|\mathcal{B}|} \right)w_2 - \frac{\eta}{|\mathcal{B}|} \sum_{i \in \mathcal{B}}x_2^{(i)} \left(x_1^{(i)} w_1 + x_2^{(i)} w_2 + b - y^{(i)}\right). \end{aligned} 
$$
可见，$L_2$范数正则化令权重$w_1$和$w_2$先自乘小于1的数，再减去不含惩罚项的梯度。因此，$L_2$范数正则化又叫权重衰减。权重衰减通过惩罚绝对值较大的模型参数为需要学习的模型增加了限制，这可能对过拟合有效。
* 丢弃法
多层感知机中神经网络图描述了一个单隐藏层的多层感知机。其中输入个数为4，隐藏单元个数为5，且隐藏单元$h_i$（$i=1, \ldots, 5$）的计算表达式为
    $$
    h_i = \phi\left(x_1 w_{1i} + x_2 w_{2i} + x_3 w_{3i} + x_4 w_{4i} + b_i\right) 
    $$
这里$\phi$是激活函数，$x_1, \ldots, x_4$是输入，隐藏单元$i$的权重参数为$w_{1i}, \ldots, w_{4i}$，偏差参数为$b_i$。当对该隐藏层使用丢弃法时，该层的隐藏单元将有一定概率被丢弃掉。设丢弃概率为$p$，那么有$p$的概率$h_i$会被清零，有$1-p$的概率$h_i$会除以$1-p$做拉伸。丢弃概率是丢弃法的超参数。具体来说，设随机变量$\xi_i$为0和1的概率分别为$p$和$1-p$。使用丢弃法时我们计算新的隐藏单元$h_i'$
$$
h_i' = \frac{\xi_i}{1-p} h_i 
$$
由于$E(\xi_i) = 1-p$，因此
$$
E(h_i') = \frac{E(\xi_i)}{1-p}h_i = h_i 
$$
即丢弃法不改变其输入的期望值。
### 代码
#### `model.train()`和`model.veal()`
`model.train()`和`model.eval()`一般在模型训练和评价的时候会加上这两句，主要是针对由于`model`在训练时和评价时`Batch Normalization`和`Dropout`方法模式不同；因此，在使用PyTorch进行训练和测试时一定注意要把实例化的`model`指定`train/eval`。  

# 梯度消失、梯度爆炸以及Kaggle房价预测
## 内容
1. 梯度消失和梯度爆炸
2. 考虑到环境因素的其他问题
3. Kaggle房价预测
## 收获
### 理论
略
### 代码
#### `Pytorch`中的参数初始化方法
[参考](https://blog.csdn.net/ys1305/article/details/94332007)