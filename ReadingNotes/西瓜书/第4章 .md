# 第4章 决策树

- [第4章 决策树](#第4章-决策树)
  - [4.1 基本流程](#41-基本流程)
  - [4.2 划分选择](#42-划分选择)
  - [4.3 剪枝处理](#43-剪枝处理)
    - [预剪枝](#预剪枝)
    - [后剪枝](#后剪枝)
  - [4.4 连续与缺失值](#44-连续与缺失值)
    - [连续值处理](#连续值处理)
    - [缺失值处理](#缺失值处理)
  - [4.5 多变量决策树](#45-多变量决策树)
  - [参考](#参考)
  

## 4.1 基本流程
pass  
## 4.2 划分选择
通过该属性，可以把样本划分得更“纯”,划分依据：  
- 信息增益  
    - ID3：信息增益越大，以为着使用属性a进行划分所获得的“纯度提升”越大  
    - 偏好：可取数值数目较多的属性有偏好  
- 信息熵  
    - C4.5:先从候选划分属性中找出信息增益高于平均水平的属性，再从中选择增益率最高的。  
    - 偏好：可取数值数目较少的属性有偏好  
- 基尼指数  
    - CART：在候选属性集合A中，选择使得划分后基尼指数最小的属性作为最优划分  
![](https://raw.githubusercontent.com/wu-zero/my_image_hosting_2019/master/img/%E5%88%92%E5%88%86%E9%80%89%E6%8B%A9.jpg)

## 4.3 剪枝处理
剪枝是决策树学习算法对付“过拟合”的主要手段，分为两种：  
- 预剪枝
- 后剪枝
### 预剪枝
**方法**：  
在决策树生成的过程中，对每个节点在划分前先进行估计，若当前节点的划分不能带来决策树泛化性能提升，则停止划分并将当前节点标记为叶节点。  
**优点**：  
    1. 降低过拟合风险  
    2. 显著减少了决策树的训练时间开销和测试时间开销  
**缺点**：  
基于“贪心”本质禁止这些分支展开，给预剪枝决策树带来了欠拟合的风险。  
### 后剪枝
**方法**：  
先从训练集生成一棵完整的决策树，然后自底向上地对非叶节点进行考察，若将该节点对应的子树替换为叶节点能带来决策树泛化能力提升，则将该子树替换为叶节点。  
**优点**：  
    1. 欠拟合风险小  
    2. 泛化能力一般优于预剪枝决策树  
**缺点**：  
训练时间开销比未剪枝决策树和预剪枝决策树都要大很多。  

ps：多看书上例子
## 4.4 连续与缺失值
### 连续值处理  
- 二分法（C4.5采用）  
- 注意：连续属性和离散属性还存在一点不一样，即若当前节点划分属性为连续属性，该属性还可作为其后代节点的划分属性。  
### 缺失值处理
- 如何在属性值确实的情况下进行划分属性选择？  
    仅根据该属性不缺失的数据来判断。  
- 给定划分属性，若样本在该属性上的值缺失，如何对样本进行划分？  
    如果属性已知，划入到对应的子节点;如果缺失，按概率划分到不同的子节点。  
![](https://raw.githubusercontent.com/wu-zero/my_image_hosting_2019/master/img/%E8%BF%9E%E7%BB%AD%E4%B8%8E%E7%BC%BA%E5%A4%B1.jpg)

## 4.5 多变量决策树
- 轴平行，相当复杂  
- ”斜决策树“，非叶节点试图建立一个合适的线性分类器  



## 参考
[南瓜书](https://datawhalechina.github.io/pumpkin-book/#/)
[【西瓜书】决策树](https://blog.csdn.net/u011607316/article/details/70667571)
[西瓜书——3.3 LDA（线性判别分析](https://www.zybuluo.com/w460461339/note/1261090)