{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "name": "#%% md\n"
        }
      },
      "source": "## 定义网络"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": "import torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\nclass Net(nn.Module):\n    \n    def __init__(self):\n        super(Net, self).__init__()\n        self.conv1 \u003d nn.Conv2d(1, 6, 5) # 输入图像channel：1, 输出channel:6,5*5卷积核\n        self.conv2 \u003d nn.Conv2d(6, 16, 5)\n        \n        self.fc1 \u003d nn.Linear(16*5*5, 120)\n        self.fc2 \u003d nn.Linear(120, 84)\n        self.fc3 \u003d nn.Linear(84, 10)\n    \n    def forward(self, x):\n        x \u003d F.max_pool2d(F.relu(self.conv1(x)), (2,2))\n        \n        x \u003d F.max_pool2d(F.relu(self.conv2(x)), 2)\n        x \u003d x.view(-1, self.num_flat_reatures(x))\n        x \u003d F.relu(self.fc1(x))\n        x \u003d F.relu(self.fc2(x))\n        x \u003d self.fc3(x)\n        return x\n    \n    def num_flat_reatures(self,x):\n        size \u003d x.size()[1:]\n        num_features \u003d 1\n        for s in size:\n            num_features *\u003d s\n        return num_features",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Net(\n  (conv1): Conv2d(1, 6, kernel_size\u003d(5, 5), stride\u003d(1, 1))\n  (conv2): Conv2d(6, 16, kernel_size\u003d(5, 5), stride\u003d(1, 1))\n  (fc1): Linear(in_features\u003d400, out_features\u003d120, bias\u003dTrue)\n  (fc2): Linear(in_features\u003d120, out_features\u003d84, bias\u003dTrue)\n  (fc3): Linear(in_features\u003d84, out_features\u003d10, bias\u003dTrue)\n)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "net \u003d Net()\nprint(net)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "10\ntorch.Size([6, 1, 5, 5])\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "params \u003d list(net.parameters())\nprint(len(params))\nprint(params[0].size())",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "tensor([[ 0.1696, -0.1008,  0.0558,  0.0315,  0.0765, -0.0610,  0.0352,  0.0404,\n         -0.1129, -0.0328]], grad_fn\u003d\u003cAddmmBackward\u003e)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "input \u003d torch.randn(1, 1, 32, 32)\nout \u003d net(input)\nprint(out)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "outputs": [],
      "source": "net.zero_grad()\nout.backward(torch.randn(1,10))",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "markdown",
      "source": "## 损失函数",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%% md\n"
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "tensor(0.7134, grad_fn\u003d\u003cMseLossBackward\u003e)\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "output \u003d net(input)\ntarget \u003d torch.randn(10)\ntarget \u003d target.view(1, -1)\ncriterion \u003d nn.MSELoss()\n\nloss \u003d criterion(output, target)\nprint(loss)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "\u003cMseLossBackward object at 0x7f2699d64160\u003e\n\u003cAddmmBackward object at 0x7f2699d64208\u003e\n\u003cAccumulateGrad object at 0x7f2699d64160\u003e\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "print(loss.grad_fn)\nprint(loss.grad_fn.next_functions[0][0])\nprint(loss.grad_fn.next_functions[0][0].next_functions[0][0])",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "conv1.bias.grad before backward\ntensor([0., 0., 0., 0., 0., 0.])\nconv1.bias.grad after backward\ntensor([ 0.0036, -0.0081,  0.0094, -0.0075,  0.0099,  0.0027])\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "net.zero_grad()\nprint(\u0027conv1.bias.grad before backward\u0027)\nprint(net.conv1.bias.grad)\n\nloss.backward()\n\nprint(\u0027conv1.bias.grad after backward\u0027)\nprint(net.conv1.bias.grad)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "outputs": [],
      "source": "import torch.optim as optim\n\n# 创建优化器（optimizer）\noptimizer \u003d optim.SGD(net.parameters(), lr\u003d0.01)\n\n# 在训练的迭代中：\noptimizer.zero_grad()   # 清零梯度缓存\noutput \u003d net(input)\nloss \u003d criterion(output, target)\nloss.backward()\noptimizer.step()    # 更新参数",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "outputs": [],
      "source": "\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n"
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}