# 课程目录
## 1. 神经网络与深度学习
- 第一周 深度学习概论
  - 1.1 欢迎
  - 1.2 什么是神剧网络
  - 1.3 用神经网络进行监督学习
  - 1.4 为什么深度学习会兴起
  - 1.5 关于这门课程
  - 1.6 课程资源
- 第二周 神经网络基础
  - 2.1 二分类
  - 2.2 logistic回归
  - 2.3 logistic回归损失函数
  - 2.4 梯度下降法
  - 2.5 导数
  - 2.6 更多导数的例子
  - 2.7 计算图
  - 2.8 计算图的导数计算
  - 2.9 logistic回归的梯度下降法
  - 2.10 m个样本的梯度下降
  - 2.11 向量化
  - 2.12 向量化的更多例子
  - 2.13 向量化logistic回归
  - 2.14 向量化logistic回归的梯度输出
  - 2.15 python中的广播
  - 2.16 关于python/numpy向量的说明
  - 2.17 jupyter/Ipython笔记本的快速指南
  - 2.18 logistic损失函数的解释
- 第三周 浅层神经网络
  - 3.1 神经网络概览
  - 3.2 神经网络表示
  - 3.3 计算神经网络的输出
  - 3.4 多个例子中的向量化
  - 3.5 向量化实现的解释
  - 3.6 激活函数
  - 3.7 为什么需要非线性激活函数
  - 3.6 激活函数的导数
  - 3.9 神经网络的梯度下降
  - 3.10 （选修）直观理解反向传播
  - 3.11 随机初始化
- 第四周 深层神经网络
  - 4.1 深层神经网络
  - 4.2 深层网络的前向传播
  - 4.3 核对矩阵的维数
  - 4.4 为什么使用深层表示
  - 4.5 搭建深层神经网络块
  - 4.6 前向和反向传播
  - 4.7 参数VS超参数
  - 4.6 这和大脑有什么关系


## 2. 改善深层神经网络：超参数调试、正则化以及优化
- 第一周 深层学习的实用
  - 1.1 训练/开发/测试集
  - 1.2 偏差/方差
  - 1.3 机器学习基础
  - 1.4 正则化
  - 1.5 为什么正则化可以减少过拟合
  - 1.6 DropOut正则化
  - 1.7 理解DropOut
  - 1.6 其他正则化方法
  - 1.9 正则化输入
  - 1.10 梯度小时与梯度爆炸
  - 1.11 神经网络的权重初始化
  - 1.12 梯度的数值逼近
  - 1.13 梯度校验
  - 1.14 关于梯度校验实现的标记
- 第二周 优化算法
  - 2.1 Minibath梯度下降法
  - 2.2 理解Minibath梯度下降法
  - 2.3 指数加权平均
  - 2.4 理解指数加权平均
  - 2.5 指数指数加权平均的偏差修正
  - 2.6 动量梯度下降法
  - 2.7 RMSprop
  - 2.6 Adam 优化算法
  - 2.9 学习率衰减
  - 2.10 局部最优问题
- 第三周 超参数调试&正则化&框架
  - 3.1 调试处理
  - 3.2 为超参数选择合适的范围
  - 3.3 超参数训练的实践:pandas VS Caviar
  - 3.4 正则化网络的激活函数
  - 3.5 将Batch Norm拟合进神经网络
  - 3.6 Batch Norm为什么凑效
  - 3.7 测试时的Batch Norm
  - 3.6 Softmax回归
  - 3.9 训练一个Softmax分类器
  - 3.10 深度学习框架
  - 3.11 TensorFlow

## 3. 结构化机器学习项目
- 第一周 机器学习策略上
  - 1.1 什么是ML策略
  - 1.2 正交化
  - 1.3 单一数字评估指标
  - 1.4 满足和优化指标
  - 1.5 训练/开发/测试集划分
  - 1.6 开发集和测试集的大小
  - 1.7 什么时候该改变开发集/测试集和指标
  - 1.6 为什么是人的表现
  - 1.9 可避免偏差
  - 1.10 理解人的表现
  - 1.11 超过人的表现
  - 1.12 改善你的模型表现
- 第二周 机器学习策略下
  - 2.1 进行误差分析
  - 2.2 清楚标记错误的数据
  - 2.3 快速搭建你的第一个系统并迭代
  - 2.4 在不同数据划分上进行训练并测试
  - 2.5 不匹配数据划分的偏差和方差
  - 2.6 定位数据不匹配
  - 2.7 迁移学习
  - 2.6 多任务学习
  - 2.9 什么是端到端的深度学习
  - 2.10 是否要使用端到端的深度学习

## 4.卷积神经网络
- 第一周 卷积神经网络
  - 1.1 计算机视觉
  - 1.2 边缘检测示例
  - 1.3 更多边缘检测内容
  - 1.4 Padding
  - 1.5 卷积步长
  - 1.6 卷积中"卷"的体现之处
  - 1.7 单层卷积网络
  - 1.6 简单卷积网络示例
  - 1.9 池化层
  - 1.10 卷积神经网络示例
  - 1.11 为什么使用卷积？
- 第二周 深层卷积神经网络实例探究
  - 2.1 为什么要进行实例探究
  - 2.2 经典网络
  - 2.3 残差网络
  - 2.4 残差网络为什么有用?
  - 2.5 网络中的网络以及1X1网络
  - 2.6 谷歌Inception网络简介
  - 2.7 Inception网络
  - 2.6 使用开源的实现方案
  - 2.9 迁移学习
  - 2.10 数据扩充
  - 2.11 计算机视觉现状
- 第三周 目标检测
  - 3.1 目标定位
  - 3.2 特征点检测
  - 3.3 目标检测
  - 3.4 卷积的滑动窗口实现
  - 3.5 BoundingBox预测
  - 3.6 并交化
  - 3.7 非极大值抑制
  - 3.6 Anchor Boxes
  - 3.9 YOLO算法
  - 3.10 (选修)RPN网络
- 第四周 人脸识别和神经风格转换
  - 4.1 什么是人脸识别？
  - 4.2 One-Shot学习
  - 4.3 Siamese网络
  - 4.4 Triplet损失
  - 4.5 面部特征与二分类
  - 4.6 什么是神经风格转换
  - 4.7 深度卷积网络在学什么
  - 4.6 代价函数
  - 4.9 内容代价函数
  - 4.10 风格损失函数
  - 4.11 一维到三维推广

