{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "collapsed": true,
        "pycharm": {
          "is_executing": false
        }
      },
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From \u003cipython-input-1-9ba188f10a13\u003e:5: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
            "WARNING:tensorflow:From /home/wyw/anaconda3/envs/tf1.13-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease write your own downloading logic.\n",
            "WARNING:tensorflow:From /home/wyw/anaconda3/envs/tf1.13-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-images-idx3-ubyte.gz\nWARNING:tensorflow:From /home/wyw/anaconda3/envs/tf1.13-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.data to implement this functionality.\n",
            "Extracting /tmp/data/train-labels-idx1-ubyte.gz\nWARNING:tensorflow:From /home/wyw/anaconda3/envs/tf1.13-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use tf.one_hot on tensors.\n",
            "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
            "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\nWARNING:tensorflow:From /home/wyw/anaconda3/envs/tf1.13-gpu/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "from __future__ import division, print_function, absolute_import\nimport tensorflow as tf\n\nfrom tensorflow.examples.tutorials.mnist import input_data\nmnist \u003d input_data.read_data_sets(\"/tmp/data/\", one_hot\u003dTrue)"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "outputs": [],
      "source": "learning_rate \u003d 0.001\nnum_steps \u003d 500\nbatch_size \u003d 128\ndisplay_step \u003d 10\n\nnum_input \u003d 784\nnum_classes \u003d 10\ndropout \u003d 0.75\n\nX \u003d tf.placeholder(tf.float32, [None, num_input])\nY \u003d tf.placeholder(tf.float32, [None, num_classes])\nkeep_prob \u003d tf.placeholder(tf.float32)",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\"\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "outputs": [],
      "source": "def conv2d(x, W, b, strides\u003d1):\n    x \u003d tf.nn.conv2d(x,W,strides\u003d[1,strides,strides,1],padding\u003d\u0027SAME\u0027)\n    x \u003d tf.nn.bias_add(x, b)\n    return  tf.nn.relu(x)\n\ndef maxpool2d(x,k\u003d2):\n    return tf.nn.max_pool(x, ksize\u003d[1,k,k,1], strides\u003d[1,k,k,1], padding\u003d\u0027SAME\u0027)\n\ndef conv_net(x, weights, biases, dropout):\n    x \u003d tf.reshape(x ,shape\u003d[-1, 28, 28, 1])\n    \n    conv1 \u003d conv2d(x, weights[\u0027wc1\u0027], biases[\u0027bc1\u0027])\n    conv1 \u003d maxpool2d(conv1, k\u003d2)\n    conv2 \u003d conv2d(conv1, weights[\u0027wc2\u0027], biases[\u0027bc2\u0027])\n    conv2 \u003d maxpool2d(conv2, k\u003d2)\n    \n    fc1 \u003d tf.reshape(conv2, [-1, weights[\u0027wd1\u0027].get_shape().as_list()[0]])\n    fc1 \u003d tf.add(tf.matmul(fc1, weights[\u0027wd1\u0027]), biases[\u0027bd1\u0027])\n    fc1 \u003d tf.nn.relu(fc1)\n    \n    fc1 \u003d tf.nn.dropout(fc1, dropout)\n    \n    out \u003d tf.add(tf.matmul(fc1, weights[\u0027out\u0027]), biases[\u0027out\u0027])\n    return out\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "WARNING:tensorflow:From /home/wyw/anaconda3/envs/tf1.13-gpu/lib/python3.6/site-packages/tensorflow/python/framework/op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\nInstructions for updating:\nColocations handled automatically by placer.\n",
            "WARNING:tensorflow:From \u003cipython-input-3-8f26e2bd9d8b\u003e:21: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\nInstructions for updating:\nPlease use `rate` instead of `keep_prob`. Rate should be set to `rate \u003d 1 - keep_prob`.\n",
            "WARNING:tensorflow:From \u003cipython-input-4-41606a4b64ba\u003e:17: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\nInstructions for updating:\n\nFuture major versions of TensorFlow will allow gradients to flow\ninto the labels input on backprop by default.\n\nSee `tf.nn.softmax_cross_entropy_with_logits_v2`.\n\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "weights \u003d {\n    \u0027wc1\u0027: tf.Variable(tf.random_normal([5, 5, 1, 32])),\n    \u0027wc2\u0027: tf.Variable(tf.random_normal([5, 5, 32, 64])),\n    \u0027wd1\u0027: tf.Variable(tf.random_normal([7*7*64, 1024])),\n    \u0027out\u0027: tf.Variable(tf.random_normal([1024, num_classes]))\n}\nbiases \u003d {\n    \u0027bc1\u0027: tf.Variable(tf.random_normal([32])),\n    \u0027bc2\u0027: tf.Variable(tf.random_normal([64])),\n    \u0027bd1\u0027: tf.Variable(tf.random_normal([1024])),\n    \u0027out\u0027: tf.Variable(tf.random_normal([num_classes]))\n}\n\nlogits \u003d conv_net(X, weights, biases, keep_prob)\nprediction \u003d tf.nn.softmax(logits)\nloss_op \u003d tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(\n    logits\u003dlogits, labels\u003dY\n))\noptimizer \u003d tf.train.AdamOptimizer(learning_rate\u003dlearning_rate)\ntrain_op \u003d optimizer.minimize(loss_op)\n\ncorrect_pred \u003d tf.equal(tf.argmax(prediction, 1), tf.argmax(Y, 1))\naccuracy \u003d tf.reduce_mean(tf.cast(correct_pred, tf.float32))\n\ninit \u003d tf.global_variables_initializer()",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "outputs": [
        {
          "name": "stdout",
          "text": [
            "Step 1, Minibatch Loss\u003d 74916.8125, Training Accuracy\u003d 0.055\nStep 10, Minibatch Loss\u003d 31245.3633, Training Accuracy\u003d 0.133\nStep 20, Minibatch Loss\u003d 9467.1211, Training Accuracy\u003d 0.492\n",
            "Step 30, Minibatch Loss\u003d 6559.4761, Training Accuracy\u003d 0.695\nStep 40, Minibatch Loss\u003d 6065.3711, Training Accuracy\u003d 0.727\nStep 50, Minibatch Loss\u003d 4880.6060, Training Accuracy\u003d 0.758\n",
            "Step 60, Minibatch Loss\u003d 4690.5479, Training Accuracy\u003d 0.789\nStep 70, Minibatch Loss\u003d 2419.7568, Training Accuracy\u003d 0.875\nStep 80, Minibatch Loss\u003d 2523.5986, Training Accuracy\u003d 0.844\n",
            "Step 90, Minibatch Loss\u003d 4856.4717, Training Accuracy\u003d 0.828\nStep 100, Minibatch Loss\u003d 2050.0083, Training Accuracy\u003d 0.898\nStep 110, Minibatch Loss\u003d 2099.4678, Training Accuracy\u003d 0.914\n",
            "Step 120, Minibatch Loss\u003d 2919.8247, Training Accuracy\u003d 0.891\nStep 130, Minibatch Loss\u003d 1005.4611, Training Accuracy\u003d 0.953\nStep 140, Minibatch Loss\u003d 2029.8369, Training Accuracy\u003d 0.898\n",
            "Step 150, Minibatch Loss\u003d 2586.4790, Training Accuracy\u003d 0.891\nStep 160, Minibatch Loss\u003d 1651.6831, Training Accuracy\u003d 0.891\nStep 170, Minibatch Loss\u003d 1357.0735, Training Accuracy\u003d 0.930\n",
            "Step 180, Minibatch Loss\u003d 2253.6536, Training Accuracy\u003d 0.883\nStep 190, Minibatch Loss\u003d 1428.6005, Training Accuracy\u003d 0.922\nStep 200, Minibatch Loss\u003d 1665.7122, Training Accuracy\u003d 0.891\n",
            "Step 210, Minibatch Loss\u003d 1562.0513, Training Accuracy\u003d 0.945\nStep 220, Minibatch Loss\u003d 2438.3328, Training Accuracy\u003d 0.883\nStep 230, Minibatch Loss\u003d 2062.8965, Training Accuracy\u003d 0.922\n",
            "Step 240, Minibatch Loss\u003d 1497.0791, Training Accuracy\u003d 0.930\nStep 250, Minibatch Loss\u003d 889.6674, Training Accuracy\u003d 0.961\nStep 260, Minibatch Loss\u003d 948.7363, Training Accuracy\u003d 0.938\n",
            "Step 270, Minibatch Loss\u003d 1363.6714, Training Accuracy\u003d 0.930\nStep 280, Minibatch Loss\u003d 1215.2697, Training Accuracy\u003d 0.930\nStep 290, Minibatch Loss\u003d 369.3766, Training Accuracy\u003d 0.969\n",
            "Step 300, Minibatch Loss\u003d 948.0016, Training Accuracy\u003d 0.938\nStep 310, Minibatch Loss\u003d 876.4315, Training Accuracy\u003d 0.938\nStep 320, Minibatch Loss\u003d 758.7606, Training Accuracy\u003d 0.953\n",
            "Step 330, Minibatch Loss\u003d 1060.3673, Training Accuracy\u003d 0.922\nStep 340, Minibatch Loss\u003d 1077.7061, Training Accuracy\u003d 0.953\nStep 350, Minibatch Loss\u003d 1426.0538, Training Accuracy\u003d 0.914\n",
            "Step 360, Minibatch Loss\u003d 333.8499, Training Accuracy\u003d 0.977\nStep 370, Minibatch Loss\u003d 554.9418, Training Accuracy\u003d 0.953\nStep 380, Minibatch Loss\u003d 518.8364, Training Accuracy\u003d 0.961\n",
            "Step 390, Minibatch Loss\u003d 890.1039, Training Accuracy\u003d 0.938\nStep 400, Minibatch Loss\u003d 453.8667, Training Accuracy\u003d 0.953\nStep 410, Minibatch Loss\u003d 921.2747, Training Accuracy\u003d 0.953\n",
            "Step 420, Minibatch Loss\u003d 639.3838, Training Accuracy\u003d 0.953\nStep 430, Minibatch Loss\u003d 1451.3560, Training Accuracy\u003d 0.961\n",
            "Step 440, Minibatch Loss\u003d 943.4768, Training Accuracy\u003d 0.945\nStep 450, Minibatch Loss\u003d 1050.3457, Training Accuracy\u003d 0.945\nStep 460, Minibatch Loss\u003d 53.1689, Training Accuracy\u003d 0.984\n",
            "Step 470, Minibatch Loss\u003d 415.8242, Training Accuracy\u003d 0.984\nStep 480, Minibatch Loss\u003d 523.6562, Training Accuracy\u003d 0.961\nStep 490, Minibatch Loss\u003d 261.5689, Training Accuracy\u003d 0.969\n",
            "Step 500, Minibatch Loss\u003d 460.0979, Training Accuracy\u003d 0.961\nOptimization Finished!\nTesting Accuracy: 0.980469\n"
          ],
          "output_type": "stream"
        }
      ],
      "source": "with tf.Session() as sess:\n    sess.run(init)\n    for step in range(1, num_steps+1):\n        batch_x, batch_y \u003d mnist.train.next_batch(batch_size)\n        sess.run(train_op, feed_dict\u003d{X: batch_x, Y: batch_y, keep_prob: dropout})\n        \n        if step%display_step \u003d\u003d 0 or step \u003d\u003d 1:\n            loss, acc \u003d sess.run([loss_op, accuracy], feed_dict\u003d{X:batch_x, Y:batch_y, keep_prob:1.0})\n            print(\"Step \" + str(step) + \", Minibatch Loss\u003d \" + \\\n                  \"{:.4f}\".format(loss) + \", Training Accuracy\u003d \" + \\\n                  \"{:.3f}\".format(acc))\n    print(\"Optimization Finished!\")    \n    print(\"Testing Accuracy:\", \\\n        sess.run(accuracy, feed_dict\u003d{X: mnist.test.images[:256],\n                                      Y: mnist.test.labels[:256],\n                                      keep_prob: 1.0}))\n",
      "metadata": {
        "pycharm": {
          "metadata": false,
          "name": "#%%\n",
          "is_executing": false
        }
      }
    }
  ],
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}